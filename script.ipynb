{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.metrics import confusion_matrix\nimport cv2 \nimport glob\nimport matplotlib.pyplot as plt\nimport tensorflow as tf \nfrom keras import applications\nfrom keras.models import Model\nfrom keras import optimizers\nfrom keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Dropout,GlobalAveragePooling2D\n#from keras import backend as K\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n#print(os.listdir(\"../input/chest_xray/chest_xray/train/NORMAL\"))\ntrain_dir_normal = \"../input/chest-xray-pneumonia/chest_xray/chest_xray/train/NORMAL/*.jpeg\"\ntrain_dir_pneumonia = \"../input/chest-xray-pneumonia/chest_xray/chest_xray/train/PNEUMONIA/*.jpeg\"\nval_dir_normal = \"../input/chest-xray-pneumonia/chest_xray/chest_xray/val/NORMAL/*.jpeg\"\nval_dir_pneumonia = \"../input/chest-xray-pneumonia/chest_xray/chest_xray/val/PNEUMONIA/*.jpeg\"\ntest_dir_normal = \"../input/chest-xray-pneumonia/chest_xray/chest_xray/test/NORMAL/*.jpeg\"\ntest_dir_pneumonia = \"../input/chest-xray-pneumonia/chest_xray/chest_xray/test/PNEUMONIA/*.jpeg\"\n# Any results you write to the current directory are saved as output.\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1e14fbadbd8b65a060d13901643d57900da6d697"},"cell_type":"code","source":"#Helper functions !! :","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"754785d8f8e40b2d551dea55555177e68368d80e"},"cell_type":"code","source":"def shuffle_in_unison(a,b):\n\tassert len(a) == len(b)\n\tp = np.random.permutation(len(a))\n\treturn a[p], b[p]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"003269afad6bca35c6b8e7c7b8be5bc1e1b86bd3"},"cell_type":"code","source":"def convert_to_one_hot(vec, num):\n    Y = np.eye(num)[vec.reshape(-1)].T\n    return Y\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"def load_data():\n    \"\"\"A function to load the whole dataset and the labels\"\"\"\n    train_data = []\n    val_data = []\n    test_data = []\n\n    train_labels = [] \n    val_labels = []\n    test_labels = []\n\n\n\n    # groupping the file names into arrays and then we're gonna read each img and put it in the above array \n    train_files_normal = glob.glob(train_dir_normal) \n    train_files_pneumonia = glob.glob(train_dir_pneumonia)\n    val_files_normal = glob.glob(val_dir_normal) \n    val_files_pneumonia = glob.glob(val_dir_pneumonia)\n\n    test_files_normal = glob.glob(test_dir_normal) \n    test_files_pneumonia = glob.glob(test_dir_pneumonia)\n\n\n\n\n    for num, file in enumerate(train_files_normal + train_files_pneumonia):\n        img = cv2.imread(file, 1) \n        img = cv2.resize(img, (150,150))\n        train_data.append(img)\n        if(num+1 <= 1341):\n            train_labels.append(0)\n        else:\n            train_labels.append(1)\n    \n    for num, file in enumerate(val_files_normal + val_files_pneumonia):\n        img = cv2.imread(file, 1)\n        img = cv2.resize(img, (150,150))\n        val_data.append(img)\n        if(num+1 <= 8):\n            val_labels.append(0)\n        else:\n            val_labels.append(1)\n    \n    for num, file in enumerate(test_files_normal + test_files_pneumonia):\n        img = cv2.imread(file, 1)\n        img = cv2.resize(img, (150,150))\n        test_data.append(img)\n        if(num+1 <= 234):\n            test_labels.append(0)\n        else:\n            test_labels.append(1)\n    \n    \n    X_train = np.array(train_data)\n    X_val  = np.array(val_data)\n    X_test = np.array(test_data)\n    \n    Y_train = np.array(train_labels)\n    Y_val = np.array(val_labels)\n    Y_test = np.array(test_labels) \n    \n    X_train, Y_train = shuffle_in_unison(X_train,Y_train)\n    X_val, Y_val = shuffle_in_unison(X_val,Y_val)\n    X_test, Y_test = shuffle_in_unison(X_test,Y_test)\n\n\n    \n    return X_train, Y_train, X_val, Y_val, X_test, Y_test \n            \n\n#print(train_labels[0:1345])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89470c1fb11ef81f3b0b9ff67d00563b70340c0a","collapsed":true},"cell_type":"code","source":"X_train,Y_train,X_val,Y_val,X_test,Y_test = load_data()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d75e8f12cbda23096b554f9b10a884415a3f8f9"},"cell_type":"code","source":"# Further preparing the data \nX_train = X_train/ 255 \nX_val = X_val/ 255 \nX_test = X_test/ 255\n\nY_train = convert_to_one_hot(Y_train, 2).T\nY_val = convert_to_one_hot(Y_val, 2).T\nY_test = convert_to_one_hot(Y_test, 2).T\n\nprint(\"shape of X_train : \" + str(X_train.shape))\nprint(\"shape of X_val : \" + str(X_val.shape))\nprint(\"shape of X_test : \" + str(X_test.shape))\nprint(\"shape of Y_train : \" + str(Y_train.shape))\nprint(\"shape of Y_val : \" + str(Y_val.shape))\nprint(\"shape of Y_test : \" + str(Y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb96abb2119a20701c84edb81c6316cb09e8b28a"},"cell_type":"code","source":"plt.imshow(X_train[150])\nprint(\"y = \" + str(np.squeeze(Y_train[150])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72349a626dbadecb5cfeaa3b73dbf78f2453b012","collapsed":true},"cell_type":"code","source":"# loading a pre-trained model(VGG16)\nbase_model = applications.VGG16(include_top = False, weights = '../input/keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2d91372a917946168328a61ac6aa592537810fe"},"cell_type":"code","source":"base_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a646b35ae6462fc43bf641eda6aaf68a4ed8eaa8","collapsed":true},"cell_type":"code","source":"sgd = optimizers.Adam(lr = 1e-4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9ee0ca11bcfeaacbdbb7a96606261bcd6266af9","collapsed":true},"cell_type":"code","source":"# now we build a function to train the model and add a bottleneck model to our pre-trained one \ndef Train_model(X_train, Y_train, X_test, Y_test, optimizer, base_model, batch_size = 64, num_epochs = 10):\n    \n    X = base_model.output\n    X = Dropout(0.5)(X)\n    X = GlobalAveragePooling2D()(X)\n    X = Dense(128, activation='relu')(X)\n    X = BatchNormalization()(X)\n    X = Dense(2, activation='sigmoid')(X)\n    model = Model(inputs = base_model.input, outputs = X)\n    \n    for layer in base_model.layers:\n        layer.trainable = False \n        \n    model.compile(optimizer = optimizer,\n                  loss = 'categorical_crossentropy',\n                  metrics = ['accuracy'])\n    \n    model.fit(X_train,Y_train,\n              batch_size = batch_size,\n              epochs = num_epochs,\n              validation_data = (X_test,Y_test))\n    \n    return model \n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea8d1d6948d41a8521dc56be8e5c3a0a300f91f6","scrolled":true},"cell_type":"code","source":"model = Train_model(X_train, Y_train, X_test, Y_test, sgd, base_model, batch_size = 64, num_epochs = 10)\nmodel.save('model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5a95885713772e6580f6dcc0aaaa2d82e77b3109"},"cell_type":"code","source":"s_model = model.save('model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5527aa0bd67e7d54ce3be41da9d52b7647d9f854"},"cell_type":"code","source":"y_pred = model.predict(X_test)\ny_pred = np.argmax(y_pred, axis = 1)\ny_true = np.argmax(Y_test, axis = 1) \nconf_mat = confusion_matrix(y_true, y_pred) \nfrom mlxtend.plotting import plot_confusion_matrix\nfig, ax = plot_confusion_matrix(conf_mat = conf_mat ,  figsize=(5, 5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8938454ac9944ff9400336f40f797e6c03ddde72"},"cell_type":"code","source":"# recall = 375 /(375+15) = .961 -> 96.1% \n# precision = 375 / (375+64) = .854 -> 85.4% \n# F1 = 2/(1/recall + 1/precision) = 90.4%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"60117704c8711c2e7ca93f37a259b3ce87768b73"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}